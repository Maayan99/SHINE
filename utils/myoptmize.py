import torch
import math
from transformers import get_linear_schedule_with_warmup

def init_optimize(grouped_params, train_loader, cfg, device): 
    optimizer = torch.optim.AdamW(grouped_params, lr=cfg.optim.learning_rate)

    total_steps = cfg.optim.num_epochs * math.ceil(len(train_loader) / max(1, cfg.run.gradient_accumulation_steps))
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=cfg.optim.warmup_steps,
        num_training_steps=total_steps,
    )

    # AMP scaler
    scaler = torch.amp.GradScaler(enabled=(cfg.run.use_amp and device.type == "cuda"))
    
    return optimizer, lr_scheduler, scaler