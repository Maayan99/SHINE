mode: train

resume_global_step: -1  # -1: don't resume,   int: resume from global steps,  latest: resume from latest
test_global_step: latest # int: test at global steps,  latest: test at latest,   best: test at best checkpoint

run:
  seed: 42
  use_fp16: false              # set false to disable AMP, or use bf16 if you extend it
  gradient_accumulation_steps: 2
  device: cuda                # auto | cuda | cpu
  use_gradient_checkpoint: false

paths:
  model_path: "./models/Qwen3-0.6B"
  output_dir: "qwen-finetuned"   # final save; checkpoints go under Hydra's run dir unless overridden

data:
  max_length: 1024
  train_batch_size: 4
  eval_batch_size: 4
  num_workers: 4
  # Switch between "mock" and "hf" when you add a real dataset loader
  source: squad

optim:
  learning_rate: 0.00005
  weight_decay: 0.01
  adapter_reg: 0.000001
  num_epochs: 1
  warmup_steps: 100
  grad_clip_norm: 0.0

pretrain:
  completion_freq: 0.5
  max_completion_ratio: 0.3
  min_completion_ratio: 0.1

logging:
  logging_steps: 1

eval:
  eval_steps: 625

save:
  save_steps: 625
  # save_best: true    # best files sometimes corrupted, disable for now

# If you use a custom local module for LoRA-wrapped Qwen, keep it here.
# Otherwise, replace with a transformers model import path and update train.py accordingly.
model:
  lora_r: 4
  num_mem_token: ${num_mem_token}
  metamodel_class_path: "LoraQwen.LoraQwen3ForCausalLM" # module.ClassName
  config_class_path: "LoraQwen.Qwen3Config" # module.ClassName
  tokenizer_from: "${paths.model_path}"
  model_from: "${paths.model_path}"

metanetwork:
  type: "transformer"
  transformer_cfg:
    encoder_cfg:
      d_model: 1024 # Fixed
      nhead: 16
      dim_feedforward: 2048
      dropout: 0
      activation: gelu
      layer_norm_eps: 0.00001
      batch_first: True
      norm_first: False
      bias: True
    num_layers: 8 # Must be even
    scale: 0.001

test:
  source: squad
  context_len: 512
  max_length: 4096
  max_new_tokens: 1024
  save_path: test
  batch_size: 16
  num_workers: 4 
  judger_model: gpt-4.1-nano
  max_concurrency: 32
  max_retries: 6

hydra:
  job:
    chdir: false
  run:
    # everyone uses the SAME dir
    dir: outputs/${now:%H-%M-%S}
  output_subdir: null  # avoid multiple `.hydra` subdirs trying to coexist

hidden_size: -1
num_layers: -1
num_mem_token: 4