resume_global_step: -1  # -1: don't resume,   int: resume from global steps,  latest: resume from latest

run:
  seed: 42
  use_fp16: false              # set false to disable AMP, or use bf16 if you extend it
  gradient_accumulation_steps: 8
  device: cuda                # auto | cuda | cpu

paths:
  model_path: "./models/Qwen3-8B"
  output_dir: "qwen-finetuned"   # final save; checkpoints go under Hydra's run dir unless overridden

data:
  max_length: 1024
  train_batch_size: 1
  eval_batch_size: 2
  num_workers: 4
  # Switch between "mock" and "hf" when you add a real dataset loader
  source: "transmla"

optim:
  learning_rate: 0.0001
  weight_decay: 0.01
  num_epochs: 1
  warmup_steps: 100
  grad_clip_norm: 0.0

logging:
  logging_steps: 1

eval:
  eval_steps: 500

save:
  save_steps: 500
  save_best: true

# If you use a custom local module for LoRA-wrapped Qwen, keep it here.
# Otherwise, replace with a transformers model import path and update train.py accordingly.
model:
  lora_r: 4
  num_mem_token: ${num_mem_token}
  metamodel_class_path: "LoraQwen.LoraQwen3ForCausalLM" # module.ClassName
  config_class_path: "LoraQwen.Qwen3Config" # module.ClassName
  tokenizer_from: "${paths.model_path}"
  model_from: "${paths.model_path}"

metanetwork:
  type: "transformer"
  transformer_cfg:
    encoder_cfg:
      d_model: 4096 # Fixed
      nhead: 32
      dim_feedforward: 8192
      dropout: 0
      activation: gelu
      layer_norm_eps: 0.00001
      batch_first: True
      norm_first: False
      bias: True
    num_layers: 4 # Must be even
    scale: 0.001

test:
  source: easy
  max_new_tokens: 1024
  save_path: test
  batch_size: 4
  num_workers: 4 
  judger_model: gpt-4.1-nano
  max_concurrency: 32
  max_retries: 6

hydra:
  job:
    chdir: false
  run:
    # everyone uses the SAME dir
    dir: outputs/${now:%H-%M-%S}
  output_subdir: null  # avoid multiple `.hydra` subdirs trying to coexist

hidden_size: -1
num_layers: -1
num_mem_token: 4

# resume_global_step: -1  # -1: don't resume,   int: resume from global steps,  latest: resume from latest

# run:
#   seed: 42
#   use_fp16: false              # set false to disable AMP, or use bf16 if you extend it
#   gradient_accumulation_steps: 1
#   device: cuda                # auto | cuda | cpu

# paths:
#   model_path: "./models/Qwen3-0.6B"
#   output_dir: "qwen-finetuned"   # final save; checkpoints go under Hydra's run dir unless overridden

# data:
#   max_length: 1024
#   train_batch_size: 4
#   eval_batch_size: 4
#   num_workers: 4
#   # Switch between "mock" and "hf" when you add a real dataset loader
#   source: "mock"

# optim:
#   learning_rate: 0.0001
#   weight_decay: 0.01
#   num_epochs: 3
#   warmup_steps: 100
#   grad_clip_norm: 0.0

# logging:
#   logging_steps: 1

# eval:
#   eval_steps: 10

# save:
#   save_steps: 10
#   save_best: true

# # If you use a custom local module for LoRA-wrapped Qwen, keep it here.
# # Otherwise, replace with a transformers model import path and update train.py accordingly.
# model:
#   lora_r: 4
#   num_mem_token: ${num_mem_token}
#   metamodel_class_path: "LoraQwen.LoraQwen3ForCausalLM" # module.ClassName
#   config_class_path: "LoraQwen.Qwen3Config" # module.ClassName
#   tokenizer_from: "${paths.model_path}"
#   model_from: "${paths.model_path}"

# metanetwork:
#   type: "transformer"
#   transformer_cfg:
#     encoder_cfg:
#       d_model: 1024 # Fixed
#       nhead: 16
#       dim_feedforward: 2048
#       dropout: 0
#       activation: gelu
#       layer_norm_eps: 0.00001
#       batch_first: True
#       norm_first: False
#       bias: True
#     num_layers: 8 # Must be even
#     scale: 0.001

# test:
#   source: easy
#   max_new_tokens: 1024
#   save_path: test
#   batch_size: 4
#   num_workers: 4 
#   judger_model: gpt-4.1-nano
#   max_concurrency: 32
#   max_retries: 6

# hydra:
#   job:
#     chdir: false
#   run:
#     # everyone uses the SAME dir
#     dir: outputs/${now:%H-%M-%S}
#   output_subdir: null  # avoid multiple `.hydra` subdirs trying to coexist

# hidden_size: -1
# num_layers: -1
# num_mem_token: 4