{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08803561",
   "metadata": {},
   "source": [
    "# SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57db11",
   "metadata": {},
   "source": [
    "### This is an example of generating a LoRA with SHINE and using the generated LoRA for multi-turn conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263e07e",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7293ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyewei/miniconda3/envs/shine/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from functools import partial\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    ")\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Specific Imports ---\n",
    "# Ensure these files exist in your directory\n",
    "try:\n",
    "    from metanetwork_family import Metanetwork\n",
    "    from utils.mydataset import HumanDataset, HumanCollator\n",
    "    from utils.myseed import set_seed\n",
    "    from utils.mylogging import get_logger\n",
    "    from utils.mysaveload import (\n",
    "        save_checkpoint,\n",
    "        load_checkpoint,\n",
    "        get_latest_checkpoint,\n",
    "    )\n",
    "    from utils.myfreeze import freeze\n",
    "    from utils.myinit import _resolve_device, _import_class\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing local modules: {e}\")\n",
    "    print(\"Please ensure you are running this notebook from the project root directory.\")\n",
    "\n",
    "# Setup Environment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Initialize simple logger for notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"notebook_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bad227",
   "metadata": {},
   "source": [
    "Set which gpu to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3333898",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74622d2f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1639f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:57:38,765 - notebook_test - INFO - Configuration loaded. Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration dictionary\n",
    "# Overrides from bash script have been applied here\n",
    "conf_dict = {\n",
    "    \"name\": \"8gpu_8lora_128metalora_lr5e-5_grouppretrain_1150\",\n",
    "    \"mode\": \"train\",\n",
    "    \"resume_global_step\": -1,\n",
    "    \"test_global_step\": \"epoch-2\", # From bash script\n",
    "    \n",
    "    \"run\": {\n",
    "        \"seed\": 42,\n",
    "        \"use_amp\": False,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"device\": \"cuda\", # auto | cuda | cpu\n",
    "        \"use_gradient_checkpoint\": False,\n",
    "    },\n",
    "    \n",
    "    \"paths\": {\n",
    "        \"model_path\": \"./models/Qwen3-8B\",\n",
    "    },\n",
    "    \n",
    "    \"data\": {\n",
    "        \"context_max_length\": 1024,\n",
    "        \"conversation_max_length\": 1024,\n",
    "        \"train_batch_size\": 1,\n",
    "        \"eval_batch_size\": 1,\n",
    "        \"num_workers\": 4,\n",
    "        \"source\": \"squad\",\n",
    "    },\n",
    "    \n",
    "    \"model\": {\n",
    "        \"lora_r\": 8,           # From bash script\n",
    "        \"metalora_r\": 128,     # From bash script\n",
    "        \"ift_additional_metalora_r\": -1,\n",
    "        \"num_mem_token\": 4,    # Placeholder, calculated later\n",
    "        \"metamodel_class_path\": \"LoraQwen.LoraQwen3ForCausalLM\",\n",
    "        \"config_class_path\": \"LoraQwen.Qwen3Config\",\n",
    "        \"tokenizer_from\": \"./models/Qwen3-8B\",\n",
    "        \"model_from\": \"./models/Qwen3-8B\",\n",
    "    },\n",
    "    \n",
    "    \"metanetwork\": {\n",
    "        \"type\": \"transformer\",\n",
    "        \"method\": \"rl\",        # From bash script\n",
    "        \"transformer_cfg\": {\n",
    "            \"encoder_cfg\": {\n",
    "                \"d_model\": 4096,\n",
    "                \"nhead\": 32,\n",
    "                \"dim_feedforward\": 8192,\n",
    "                \"dropout\": 0,\n",
    "                \"activation\": \"gelu\",\n",
    "                \"layer_norm_eps\": 0.00001,\n",
    "                \"batch_first\": True,\n",
    "                \"norm_first\": False,\n",
    "                \"bias\": True\n",
    "            },\n",
    "            \"couple_encoder_cfg\": {\n",
    "                \"d_model\": 4096,\n",
    "                \"nhead\": 32,\n",
    "                \"dim_feedforward\": 8192,\n",
    "                \"dropout\": 0,\n",
    "                \"activation\": \"gelu\",\n",
    "                \"layer_norm_eps\": 0.00001,\n",
    "                \"batch_first\": True,\n",
    "                \"norm_first\": False,\n",
    "                \"bias\": True\n",
    "            },\n",
    "            \"layer_transformer_first\": True,\n",
    "            \"mean_pool_size\": 1,\n",
    "            \"num_layers\": 4,    # From bash script\n",
    "            \"couple_num_layers\": 0,\n",
    "            \"scale\": 0.001\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    \"test\": {\n",
    "        \"context_avg_len\": 512,\n",
    "        \"context_max_length\": 1550,      # From bash script\n",
    "        \"conversation_max_length\": 5000, # From bash script\n",
    "        \"max_new_tokens\": 128,\n",
    "    },\n",
    "    \n",
    "    \"hidden_size\": -1,\n",
    "    \"num_layers\": -1,\n",
    "    \"num_mem_token\": 4\n",
    "}\n",
    "\n",
    "# Convert dict to OmegaConf object to match original code access patterns (cfg.model.lora_r)\n",
    "cfg = OmegaConf.create(conf_dict)\n",
    "logger.info(f\"Configuration loaded. Seed: {cfg.run.seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce08345",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b35dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_think_and_answer(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Robustly splits model output into (think_part, answer_part).\n",
    "    Handles cases with missing tags or incomplete generation.\n",
    "    \"\"\"\n",
    "    think = \"\"\n",
    "    answer = text\n",
    "    \n",
    "    # Check for DeepSeek/Qwen style thinking tags\n",
    "    if \"<think>\" in text:\n",
    "        parts = text.split(\"<think>\", 1)\n",
    "        # Content before <think> is usually empty or irrelevant, but we focus on what's after\n",
    "        rest = parts[1]\n",
    "        \n",
    "        if \"</think>\" in rest:\n",
    "            think_part, answer_part = rest.split(\"</think>\", 1)\n",
    "            think = think_part.strip()\n",
    "            answer = answer_part.strip()\n",
    "        else:\n",
    "            # Thinking block started but didn't close (incomplete generation)\n",
    "            think = rest.strip()\n",
    "            answer = \"\" # Or handle as [error]\n",
    "    else:\n",
    "        # No thinking tags found\n",
    "        answer = text.strip()\n",
    "\n",
    "    # Clean up standard answer prefixes if necessary\n",
    "    answer = re.sub(r\"^(final answer|answer)\\s*:\\s*\", \"\", answer, flags=re.IGNORECASE).strip()\n",
    "    \n",
    "    return think, answer\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_multiturn(\n",
    "    metanetwork,\n",
    "    dataloader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    use_metanet: bool = True,\n",
    "    metalora: Optional[torch.Tensor] = None,\n",
    "    max_new_tokens: int = 500,\n",
    "    max_conversation_length: int = 3000,\n",
    "):\n",
    "    metanetwork.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Iterate with progress bar\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Generating\"):\n",
    "        questions = batch['questions'][0]\n",
    "        \n",
    "        # Handle initial messages\n",
    "        initial_msg = batch[\"initial_messages\"][0]\n",
    "        # Check if initial_msg is effectively empty/None (depending on how collation works)\n",
    "        if isinstance(initial_msg, dict) and initial_msg:\n",
    "            messages = [initial_msg]\n",
    "        else:\n",
    "            messages = []\n",
    "            \n",
    "        evidence = batch[\"evidence\"][0]\n",
    "        evidence_ids = batch[\"evidence_ids\"].to(device, non_blocking=True)\n",
    "        evidence_attention_mask = batch[\"evidence_attention_mask\"].to(device, non_blocking=True)\n",
    "        \n",
    "        lora_dict = None\n",
    "        if use_metanet:\n",
    "            # Generate the LoRA weights using the meta-network based on the evidence\n",
    "            lora_dict = metanetwork.generate_lora_dict(evidence_ids, evidence_attention_mask, metalora)\n",
    "        \n",
    "        conversation_log = [{\"initial message\": deepcopy(messages)}]\n",
    "        error_count_local = 0\n",
    "        \n",
    "        for q_idx, question in enumerate(questions):\n",
    "            messages.append({\"role\": \"user\", \"content\": question})\n",
    "            \n",
    "            # Use chat template\n",
    "            input_enc = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_conversation_length,\n",
    "                truncation=True,\n",
    "                return_dict=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            \n",
    "            input_ids = input_enc[\"input_ids\"].to(device)\n",
    "            attention_mask = input_enc[\"attention_mask\"].to(device)\n",
    "            \n",
    "            gen_kwargs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": tokenizer.eos_token_id,\n",
    "                \"do_sample\": False,\n",
    "                \"ignore_mem_token\": True, \n",
    "                \"loradict\": lora_dict,    # Passing the dynamic LoRA weights\n",
    "            }\n",
    "            \n",
    "            # Generate\n",
    "            # Note: Assuming metamodel.generate supports'loradict' via your custom implementation\n",
    "            outputs = metanetwork.metamodel.generate(**gen_kwargs)\n",
    "            \n",
    "            # Decode\n",
    "            new_tokens = outputs[0, input_ids.shape[1]:]\n",
    "            think_answer_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            think_text, answer_text = extract_think_and_answer(think_answer_text)\n",
    "            \n",
    "            if not think_text and not answer_text:\n",
    "                error_count_local += 1\n",
    "                think_text = \"[error]\"\n",
    "            \n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer_text})\n",
    "\n",
    "            conversation_log.append({\n",
    "                \"turn\": q_idx + 1,\n",
    "                \"question\": question,\n",
    "                \"think\": think_text,\n",
    "                \"answer\": answer_text,\n",
    "            })\n",
    "            \n",
    "        conversation_log[0][\"error_count\"] = error_count_local\n",
    "        results.append(conversation_log)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550ad9c",
   "metadata": {},
   "source": [
    "## Model Initialization & Checkpoint Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae38c6",
   "metadata": {},
   "source": [
    "Here we load the model after mqa instruction fine-tuning for multi-turn conversations.\n",
    "If you want to better 1-turn conversation performance, modify the resume path to load the model after mqa and 1qa instruction fine-tuning in step 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07aa0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/liuyewei/SHINE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:57:39,178 - notebook_test - INFO - Using device: cuda\n",
      "2026-02-08 23:57:39,179 - notebook_test - INFO - Loading model classes...\n",
      "2026-02-08 23:57:39,644 - notebook_test - INFO - Loading config from ./models/Qwen3-8B\n",
      "2026-02-08 23:57:39,648 - notebook_test - INFO - Calculating memory tokens for Transformer Metanetwork...\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.59s/it]\n",
      "2026-02-08 23:57:53,937 - notebook_test - INFO - Set num_mem_token to 148\n",
      "2026-02-08 23:57:57,330 - notebook_test - INFO - Loading main model...\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.58s/it]\n",
      "Some weights of LoraQwen3ForCausalLM were not initialized from the model checkpoint at ./models/Qwen3-8B and are newly initialized: ['model.mem_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2026-02-08 23:58:30,618 - notebook_test - INFO - Initializing Metanetwork...\n",
      "2026-02-08 23:59:28,872 - notebook_test - INFO - Attempting to load checkpoint from: checkpoints/8gpu_8lora_128metalora_lr5e-5_grouppretrain_1150/iftpwc/checkpoint-epoch-2\n",
      "2026-02-09 00:00:55,150 - notebook_test - INFO - Checkpoint loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# 1. Setup Seed and Device\n",
    "set_seed(int(cfg.run.seed))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg.run.device == \"cuda\" else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Dynamic Import of Model Classes\n",
    "logger.info(\"Loading model classes...\")\n",
    "try:\n",
    "    MetaModelCls = _import_class(cfg.model.metamodel_class_path)\n",
    "    ConfigCls = _import_class(cfg.model.config_class_path)\n",
    "except Exception as e:\n",
    "    raise ImportError(f\"Could not import model classes: {cfg.model.metamodel_class_path}. Error: {e}\")\n",
    "\n",
    "# 3. Load Config\n",
    "logger.info(f\"Loading config from {cfg.model.model_from}\")\n",
    "config = ConfigCls.from_pretrained(cfg.model.model_from)\n",
    "config.num_mem_token = -1\n",
    "cfg.hidden_size = config.hidden_size\n",
    "cfg.num_layers = config.num_hidden_layers\n",
    "\n",
    "# 4. Calculate num_mem_token for Transformer Metanetwork\n",
    "logger.info(\"Calculating memory tokens for Transformer Metanetwork...\")\n",
    "# Temporarily load model to calculate params\n",
    "tmp_model = MetaModelCls.from_pretrained(cfg.model.model_from, config=config)\n",
    "\n",
    "lora_params = tmp_model.lora_params_numel(cfg.model.lora_r)\n",
    "base_params = cfg.hidden_size * cfg.num_layers\n",
    "\n",
    "assert lora_params % base_params == 0, \\\n",
    "    f\"lora_params ({lora_params}) must be divisible by hidden*layers ({base_params})\"\n",
    "\n",
    "config.num_mem_token = lora_params // base_params\n",
    "cfg.num_mem_token = config.num_mem_token\n",
    "del tmp_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "logger.info(f\"Set num_mem_token to {config.num_mem_token}\")\n",
    "\n",
    "# 5. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model.tokenizer_from, padding_side=\"left\", use_fast=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.tokenizer_from, padding_side=\"left\", use_fast=True)\n",
    "tokenizer.add_tokens(['<RECON>', '<COMP>', '<NOTHING>'])\n",
    "tokenizer.chat_template = \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0].role == 'system' %}\\n        {{- messages[0].content + '\\\\n\\\\n' }}\\n    {%- endif %}\\n    {{- \\\"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0].role == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0].content + '<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \\\"user\\\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = '' %}\\n    {%- endif %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + content + '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {%- set reasoning_content = '' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if '</think>' in content %}\\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\\\n').split('<think>')[-1].lstrip('\\\\n') %}\\n                {%- set content = content.split('</think>')[-1].lstrip('\\\\n') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if (loop.last or (not loop.last and reasoning_content)) and (enable_thinking is not defined or enable_thinking != false) %}\\n                {{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\\n            {%- else %}\\n                {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- '<|im_start|>' + message.role + '\\\\n' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- '\\\\n' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- '<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n                {{- tool_call.name }}\\n                {{- '\\\", \\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- '}\\\\n</tool_call>' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n    {%- if enable_thinking is not defined or enable_thinking != false %}\\n        {{- '<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\"\n",
    "    \n",
    "\n",
    "# 6. Load Actual Model\n",
    "logger.info(\"Loading main model...\")\n",
    "metamodel = MetaModelCls.from_pretrained(cfg.model.model_from, config=config)\n",
    "metamodel.reset_mem_tokens()\n",
    "metamodel.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "# 7. Initialize Metanetwork\n",
    "logger.info(\"Initializing Metanetwork...\")\n",
    "metanetwork = Metanetwork(metamodel, cfg, metamodel.lora_params_numel(cfg.model.lora_r))\n",
    "metanetwork.to(device)\n",
    "freeze(metamodel) # Freeze base model, train/use metanetwork\n",
    "\n",
    "# 8. Load Checkpoint\n",
    "# Here we load the model after mqa instruction fine-tuning for multi-turn conversations.\n",
    "# If you want to better 1-turn conversation performance, modify the resume path to load the model after mqa and 1qa instruction fine-tuning.\n",
    "ckpt_root = os.path.join(\"checkpoints\", f\"{cfg.name}\", \"iftpwc\")\n",
    "resume_dir = os.path.join(ckpt_root, f\"checkpoint-{cfg.test_global_step}\")\n",
    "\n",
    "logger.info(f\"Attempting to load checkpoint from: {resume_dir}\")\n",
    "\n",
    "if os.path.exists(resume_dir):\n",
    "    metanetwork, metalora, _ = load_checkpoint(metanetwork, resume_dir, device)\n",
    "    logger.info(\"Checkpoint loaded successfully.\")\n",
    "else:\n",
    "    logger.warning(f\"Checkpoint directory {resume_dir} not found! Running with initialized weights.\")\n",
    "    metalora = None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a837ca",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f7775",
   "metadata": {},
   "source": [
    "You can change the data to try on any different contexts and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511e423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 00:00:55,176 - notebook_test - INFO - Preparing Data...\n",
      "2026-02-09 00:00:55,180 - notebook_test - INFO - Data Loader ready.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Preparing Data...\")\n",
    "\n",
    "# Change the contexts and questions as you like.\n",
    "data = [{\"context\": \"Apple is green.\", \"questions\":[\"What color is an apple?\", \"What color is a banana\"]},\n",
    "        {\"context\": \"Chinese food is the best food on earth.\", \"questions\":[\"Which food is the best?\", \"What do you want to eat?\"]},\n",
    "        {\"context\": \"If the light is on, somebody must be at home. If the light is off, often nobody is at home. But this holds true only during the day. In the night people are all sleeping so there will always be no lights.\",\n",
    "         \"questions\":[\"What does it mean if the light is on?\", \"What does it mean if the light is off?\", \"Why in the night this rule might not hold true?\"]},\n",
    "         {\"context\": \"Whoever organizes cheating in a national examination prescribed by law shall be sentenced to fixed-term imprisonment of not more than three years or criminal detention and shall also be fined, or shall be fined only; if the circumstances are serious, he shall be sentenced to fixed-term imprisonment of not less than three years but not more than seven years and shall also be fined.\",\n",
    "         \"questions\":[\"What will happen if one organize cheating?\", \"How long will one be imprisoned if he cheated in an exam and the situation is very serious?\"]},\n",
    "         {\"context\": \"When someone says \\\"fair enough\\\", it can mean two slightly different things, and you usually understand which one it is from the tone and the moment. Sometimes it means real agreement — the person has heard your reason, it makes sense to them, and they are genuinely okay with it. Other times, it does not mean they agree at all. It is more like a polite way of saying, \\\"I do not think the same, but I am done arguing.\\\" In that case, fair enough is about keeping the conversation calm and moving on, not about changing their mind.\",\n",
    "          \"questions\":[\"What does \\\"fair enough\\\" mean?\", \"Does \\\"fair enough\\\" have the agree meaning?\", \"Does \\\"fair enough\\\" have disagree meaning?\", \"OK, fair enough.\"]}]\n",
    "\n",
    "\n",
    "human_dataset = HumanDataset(data)\n",
    "\n",
    "test_collator = HumanCollator(tokenizer, context_max_length=cfg.test.context_max_length, conversation_max_length=cfg.test.conversation_max_length, cfg=cfg)\n",
    "test_prompt_collator = HumanCollator(tokenizer, context_max_length=cfg.test.context_max_length, conversation_max_length=cfg.test.conversation_max_length, cfg=cfg, sys_msg=True)\n",
    "test_only_question_collator = HumanCollator(tokenizer, context_max_length=cfg.test.context_max_length, conversation_max_length=cfg.test.conversation_max_length, cfg=cfg, sys_msg=True, no_evidence=True)\n",
    "\n",
    "generate_test_loader = DataLoader(\n",
    "    human_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "generate_prompt_test_loader = DataLoader(\n",
    "    human_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_prompt_collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "generate_only_question_test_loader = DataLoader(\n",
    "    human_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_only_question_collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(\"Data Loader ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2583995",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac5e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 00:00:55,197 - notebook_test - INFO - Starting Inference...\n",
      "Generating:   0%|          | 0/5 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating: 100%|██████████| 5/5 [00:39<00:00,  7.82s/it]\n",
      "Generating: 100%|██████████| 5/5 [00:43<00:00,  8.67s/it]\n",
      "Generating: 100%|██████████| 5/5 [00:24<00:00,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Conversation 1 ---\n",
      "Initial message:\n",
      "[SHINE         ]: []\n",
      "[In-Context    ]: [{'role': 'system', 'content': 'You are a helpful assistant, answer the questions based on the given context. Each answer must be directly extractable from the context (i.e., an exact span or minor paraphrase for fluency). Do not invent information. Answer the question directly and output nothing else. Never enter think mode.\\n\\nContext: Apple is green.'}]\n",
      "[Only Question ]: [{'role': 'system', 'content': \"You are a helpful assistant. Answer the question concisely with short words or phrases. Answer the question directly and output nothing else. Never say you don't know the answer. Never enter think mode.\"}]\n",
      "Context: Apple is green.\n",
      "Turn: 1\n",
      "Question: What color is an apple?\n",
      "[SHINE         ]: An apple can be green, red, or yellow, depending on the variety.\n",
      "[In-Context    ]: An apple is green.\n",
      "[Only Question ]: Red.\n",
      "Turn: 2\n",
      "Question: What color is a banana\n",
      "[SHINE         ]: A banana is typically yellow when ripe.\n",
      "[In-Context    ]: The context does not provide information about the color of a banana.\n",
      "[Only Question ]: Yellow.\n",
      "\n",
      "\n",
      "\n",
      "--- Conversation 2 ---\n",
      "Initial message:\n",
      "[SHINE         ]: []\n",
      "[In-Context    ]: [{'role': 'system', 'content': 'You are a helpful assistant, answer the questions based on the given context. Each answer must be directly extractable from the context (i.e., an exact span or minor paraphrase for fluency). Do not invent information. Answer the question directly and output nothing else. Never enter think mode.\\n\\nContext: Chinese food is the best food on earth.'}]\n",
      "[Only Question ]: [{'role': 'system', 'content': \"You are a helpful assistant. Answer the question concisely with short words or phrases. Answer the question directly and output nothing else. Never say you don't know the answer. Never enter think mode.\"}]\n",
      "Context: Chinese food is the best food on earth.\n",
      "Turn: 1\n",
      "Question: Which food is the best?\n",
      "[SHINE         ]: Chinese food\n",
      "[In-Context    ]: Chinese food is the best food on earth.\n",
      "[Only Question ]: It's subjective. Personal preference varies.\n",
      "Turn: 2\n",
      "Question: What do you want to eat?\n",
      "[SHINE         ]: I want to eat Chinese food.\n",
      "[In-Context    ]: I don't have personal preferences or the ability to want to eat.\n",
      "[Only Question ]: I don't eat food.\n",
      "\n",
      "\n",
      "\n",
      "--- Conversation 3 ---\n",
      "Initial message:\n",
      "[SHINE         ]: []\n",
      "[In-Context    ]: [{'role': 'system', 'content': 'You are a helpful assistant, answer the questions based on the given context. Each answer must be directly extractable from the context (i.e., an exact span or minor paraphrase for fluency). Do not invent information. Answer the question directly and output nothing else. Never enter think mode.\\n\\nContext: If the light is on, somebody must be at home. If the light is off, often nobody is at home. But this holds true only during the day. In the night people are all sleeping so there will always be no lights.'}]\n",
      "[Only Question ]: [{'role': 'system', 'content': \"You are a helpful assistant. Answer the question concisely with short words or phrases. Answer the question directly and output nothing else. Never say you don't know the answer. Never enter think mode.\"}]\n",
      "Context: If the light is on, somebody must be at home. If the light is off, often nobody is at home. But this holds true only during the day. In the night people are all sleeping so there will always be no lights.\n",
      "Turn: 1\n",
      "Question: What does it mean if the light is on?\n",
      "[SHINE         ]: If the light is on, somebody must be at home.\n",
      "[In-Context    ]: If the light is on, somebody must be at home.\n",
      "[Only Question ]: It means the device is powered on.\n",
      "Turn: 2\n",
      "Question: What does it mean if the light is off?\n",
      "[SHINE         ]: If the light is off, often nobody is at home.\n",
      "[In-Context    ]: If the light is off, often nobody is at home.\n",
      "[Only Question ]: It means the device is powered off.\n",
      "Turn: 3\n",
      "Question: Why in the night this rule might not hold true?\n",
      "[SHINE         ]: During the night people are all sleeping.\n",
      "[In-Context    ]: Because in the night people are all sleeping so there will always be no lights.\n",
      "[Only Question ]: Because light sources may be dim or absent.\n",
      "\n",
      "\n",
      "\n",
      "--- Conversation 4 ---\n",
      "Initial message:\n",
      "[SHINE         ]: []\n",
      "[In-Context    ]: [{'role': 'system', 'content': 'You are a helpful assistant, answer the questions based on the given context. Each answer must be directly extractable from the context (i.e., an exact span or minor paraphrase for fluency). Do not invent information. Answer the question directly and output nothing else. Never enter think mode.\\n\\nContext: Whoever organizes cheating in a national examination prescribed by law shall be sentenced to fixed-term imprisonment of not more than three years or criminal detention and shall also be fined, or shall be fined only; if the circumstances are serious, he shall be sentenced to fixed-term imprisonment of not less than three years but not more than seven years and shall also be fined.'}]\n",
      "[Only Question ]: [{'role': 'system', 'content': \"You are a helpful assistant. Answer the question concisely with short words or phrases. Answer the question directly and output nothing else. Never say you don't know the answer. Never enter think mode.\"}]\n",
      "Context: Whoever organizes cheating in a national examination prescribed by law shall be sentenced to fixed-term imprisonment of not more than three years or criminal detention and shall also be fined, or shall be fined only; if the circumstances are serious, he shall be sentenced to fixed-term imprisonment of not less than three years but not more than seven years and shall also be fined.\n",
      "Turn: 1\n",
      "Question: What will happen if one organize cheating?\n",
      "[SHINE         ]: He shall be sentenced to fixed-term imprisonment of not more than three years or criminal detention and shall also be fined\n",
      "[In-Context    ]: If one organizes cheating, they will be sentenced to fixed-term imprisonment of not more than three years or criminal detention and shall also be fined, or shall be fined only; if the circumstances are serious, they will be sentenced to fixed-term imprisonment of not less than three years but not more than seven years and shall also be fined.\n",
      "[Only Question ]: It is illegal and unethical. You could face serious consequences, including fines, imprisonment, and damage to your reputation.\n",
      "Turn: 2\n",
      "Question: How long will one be imprisoned if he cheated in an exam and the situation is very serious?\n",
      "[SHINE         ]: Three years or more but not more than seven years\n",
      "[In-Context    ]: If the situation is very serious, one will be imprisoned for not less than three years but not more than seven years.\n",
      "[Only Question ]: The imprisonment duration depends on the jurisdiction and severity of the offense, but it can range from several months to several years.\n",
      "\n",
      "\n",
      "\n",
      "--- Conversation 5 ---\n",
      "Initial message:\n",
      "[SHINE         ]: []\n",
      "[In-Context    ]: [{'role': 'system', 'content': 'You are a helpful assistant, answer the questions based on the given context. Each answer must be directly extractable from the context (i.e., an exact span or minor paraphrase for fluency). Do not invent information. Answer the question directly and output nothing else. Never enter think mode.\\n\\nContext: When someone says \"fair enough\", it can mean two slightly different things, and you usually understand which one it is from the tone and the moment. Sometimes it means real agreement — the person has heard your reason, it makes sense to them, and they are genuinely okay with it. Other times, it does not mean they agree at all. It is more like a polite way of saying, \"I do not think the same, but I am done arguing.\" In that case, fair enough is about keeping the conversation calm and moving on, not about changing their mind.'}]\n",
      "[Only Question ]: [{'role': 'system', 'content': \"You are a helpful assistant. Answer the question concisely with short words or phrases. Answer the question directly and output nothing else. Never say you don't know the answer. Never enter think mode.\"}]\n",
      "Context: When someone says \"fair enough\", it can mean two slightly different things, and you usually understand which one it is from the tone and the moment. Sometimes it means real agreement — the person has heard your reason, it makes sense to them, and they are genuinely okay with it. Other times, it does not mean they agree at all. It is more like a polite way of saying, \"I do not think the same, but I am done arguing.\" In that case, fair enough is about keeping the conversation calm and moving on, not about changing their mind.\n",
      "Turn: 1\n",
      "Question: What does \"fair enough\" mean?\n",
      "[SHINE         ]: \"Fair enough\" means two slightly different things, and you usually understand which one the person is saying,\n",
      "[In-Context    ]: \"Fair enough\" can mean either real agreement, where the person has heard your reason, it makes sense to them, and they are genuinely okay with it, or it can be a polite way of saying, \"I do not think the same, but I am done arguing,\" which is about keeping the conversation calm and moving on, not about changing their mind.\n",
      "[Only Question ]: It means something is acceptable or reasonable.\n",
      "Turn: 2\n",
      "Question: Does \"fair enough\" have the agree meaning?\n",
      "[SHINE         ]: It can mean real agreement — the person has heard your reason, it makes sense to them, and they are done arguing\n",
      "[In-Context    ]: Yes, \"fair enough\" can have the meaning of real agreement, where the person has heard your reason, it makes sense to them, and they are genuinely okay with it.\n",
      "[Only Question ]: Yes, it can mean agreement.\n",
      "Turn: 3\n",
      "Question: Does \"fair enough\" have disagree meaning?\n",
      "[SHINE         ]: It can mean a polite way of saying, \"I do not think the same, but I am not going to argue about it.\"\n",
      "[In-Context    ]: Yes, \"fair enough\" can have the meaning of disagreement, where it is a polite way of saying, \"I do not think the same, but I am done arguing,\" and it is about keeping the conversation calm and moving on, not about changing their mind.\n",
      "[Only Question ]: No, it does not mean disagree.\n",
      "Turn: 4\n",
      "Question: OK, fair enough.\n",
      "[SHINE         ]: Fair enough\n",
      "[In-Context    ]: Yes, \"fair enough\" can mean either real agreement or a polite way of indicating disagreement while ending the discussion.\n",
      "[Only Question ]: You're welcome.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting Inference...\")\n",
    "\n",
    "# Run generation\n",
    "# metalora is loaded from checkpoint in Cell 4\n",
    "\n",
    "results = generate_multiturn(\n",
    "    metanetwork,\n",
    "    generate_test_loader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    use_metanet=True,\n",
    "    metalora=metalora,\n",
    "    max_new_tokens=500,\n",
    "    max_conversation_length=3000,\n",
    ")\n",
    "\n",
    "results_in_context = generate_multiturn(\n",
    "    metanetwork,\n",
    "    generate_prompt_test_loader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    use_metanet=False,\n",
    "    max_new_tokens=500,\n",
    "    max_conversation_length=3000,\n",
    ")\n",
    "\n",
    "results_only_question = generate_multiturn(\n",
    "    metanetwork,\n",
    "    generate_only_question_test_loader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    use_metanet=False,\n",
    "    max_new_tokens=500,\n",
    "    max_conversation_length=3000,\n",
    ")\n",
    "for i in range(len(results)):\n",
    "    print(f\"--- Conversation {i + 1} ---\")\n",
    "    print(f\"Initial message:\")\n",
    "    print(f\"[{\"SHINE\":<14}]: {results[i][0][\"initial message\"]}\\n[{'In-Context':<14}]: {results_in_context[i][0][\"initial message\"]}\\n[{'Only Question':<14}]: {results_only_question[i][0][\"initial message\"]}\")\n",
    "    print(f\"Context: {data[i]['context']}\") \n",
    "    for j in range(1, len(results[i])):\n",
    "        print(f\"Turn: {j}\")\n",
    "        print(f\"Question: {results[i][j][\"question\"]}\")\n",
    "        print(f\"[{'SHINE':<14}]: {results[i][j]['answer']}\")\n",
    "        print(f\"[{'In-Context':<14}]: {results_in_context[i][j]['answer']}\")\n",
    "        print(f\"[{'Only Question':<14}]: {results_only_question[i][j]['answer']}\")\n",
    "    print(f\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
